{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:22:53.496314700Z",
     "start_time": "2024-02-20T14:22:46.582037Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "import torch\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-20T14:22:54.650810600Z",
     "start_time": "2024-02-20T14:22:53.497314500Z"
    }
   },
   "id": "c2f0d11cd1859b3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "documents = (\n",
    "    SimpleDirectoryReader(\"C:/Users/skfrqt/Desktop/jk/milenium3/documents\", recursive=True, required_exts=[\".md\", \".pdf\"]).load_data())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-20T14:22:54.651770700Z"
    }
   },
   "id": "2db9908bbce23d06",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "selected_model = \"TheBloke/Llama-2-7B-Chat-GPTQ\" #pc keeps crashing whenever i use 13b. testing aqlm, aql, gguf and they were slow (vllm speeds it but set-up is a hell on earth) GPTQ is a nice balance. If getting CUDA OUT OF MEMORY, try switching smaller params. WIll be a bit slower but runs on most devices\n",
    "    \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dd5c9590e228fb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM( #play around with the llm parameters here. every number here is either random or from docs - i have not tested new values\n",
    "    context_window=3900,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": False}, #need to test different values for this\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"cuda\",\n",
    "    tokenizer_kwargs={\"max_length\": 3900},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "llm.model_kwargs = {\"torch_dtype\": torch.float16}  # Use lower precision (if supported)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1434cec8dd69555c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e258eea6176f567d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"math_docs\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a5919ad687e53d79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"math_docs\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,   \n",
    "    embed_model=embed_model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4be595749d0cde3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine() \n",
    "response = query_engine.query(\"Tell me the rules of multiplying matrices if they are not square matrices\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a748659fe2fdd009",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "460cd0646562520d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
