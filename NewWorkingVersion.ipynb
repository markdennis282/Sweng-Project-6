{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import torch\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding #10-20x speed thank you!!!\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f4caeeb79afc1e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "selected_model = \"TheBloke/Llama-2-7B-Chat-GPTQ\" #pc keeps crashing whenever i use 13b. tested aqlm, aql, gguf and they were slow (vllm speeds it but set-up is a hell on earth) GPTQ is a nice balance. If getting CUDA OUT OF MEMORY, try switching smaller params. WIll be a bit slower but runs on most devices\n",
    "    \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant providing responses in a professional North American business context. Follow these rules:\n",
    "- Ensure clarity and readability in all responses\n",
    "- Respond precisely to the specific query without extraneous text.\n",
    "- Maintain a professional, business-oriented tone.\n",
    "- Do not use filler phrases or references to the documentation sources such as \"based on the context, from the document source, etc\"\n",
    "- Do not use offensive or inappropriate language.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST] \"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb319757f08aebc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "llm = HuggingFaceLLM( #play around with the llm parameters here. Room for improvement.\n",
    "    context_window=3200,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.1}, #need to test different values for this\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=selected_model,\n",
    "    model_name=selected_model,\n",
    "    device_map=\"cuda\", #if CUDA OUT OF MEMORY, switch to \"auto\". Will offload some of the work to CPU. But, will be so much slower. Try reducing the context_window and max_new_tokens if in a time crunch. \n",
    "    tokenizer_kwargs={\"max_length\": 3200},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    "    \n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a46e82be13ae1d8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eca58c29ff6b686e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents = (\n",
    "    SimpleDirectoryReader(\"C:/Users/Adel/Desktop/AWS_FULL_DOCS\", recursive=True, required_exts=[\".md\"]).load_data())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f052c6920168d2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#This cell is needed for nodes to be created. You can skip this cell if you don't want to use the hybrid retriever. If you do, then all nodes must be created again. \n",
    "\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() #Fix for a jupyter specific runtime error. Might not be needed with full BE implementation\n",
    "\n",
    "#https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\n",
    "#this is default but I've included it here to allow for easy modification\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", chunk_size=1024, chunk_overlap=20\n",
    ")\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[text_splitter]\n",
    ")\n",
    "\n",
    "nodes = pipeline.run(\n",
    "    documents=documents,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "#TODO: Add back metadata (title, questionanswers, etc) to the nodes. They were removed as they were MIND NUMBINGLY slow but improve performance. For the demo, we can do without them but for the final product, they are a must.\n",
    "#TODO: Add persistent memory storage for nodes. They are needed for the hybrid retriever and is not feasible to have to run the ingestion pipeline every time (although it only takes a few minutes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9584ce7620ce9422"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#DO NOT RUN THIS CELL IF YOU HAVE ALREADY CREATED THE EMBEDDINGS. RUN THE FOLLOWING ONE INSTEAD\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"aws_documentation_test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context, embed_model=embed_model, show_progress=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28cbb00359111b03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loads the embeddings from chromaDB into index\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"aws_documentation_test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af7c5b9dcf23d54b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#https://medium.com/@trent.niemeyer/10k-gpt-upgrading-fb94603cc38b\n",
    "#https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever.html\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# retrieves the top 10 most similar nodes using embeddings\n",
    "vector_retriever = index.as_retriever(similarity_top_k=10)\n",
    "\n",
    "# retrieves the top 10 most similar nodes using bm25\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f923d2875c259941"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "\n",
    "class HybridRetriever(BaseRetriever):\n",
    "    def __init__(self, vector_retriever, bm25_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.bm25_retriever = bm25_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query, **kwargs):\n",
    "        bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "        vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "\n",
    "        # combine the two lists of nodes\n",
    "        all_nodes = []\n",
    "        node_ids = set()\n",
    "        for n in bm25_nodes + vector_nodes:\n",
    "            if n.node.node_id not in node_ids:\n",
    "                all_nodes.append(n)\n",
    "                node_ids.add(n.node.node_id)\n",
    "        return all_nodes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd6a8c34dd91f381"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62558b4342b6d004"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "reranker = SentenceTransformerRerank(top_n=4, model=\"BAAI/bge-reranker-base\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c43b58bb6cfc313"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/Adel/Downloads/QA_true.csv\")\n",
    "if 'Generated_answer' not in df.columns:\n",
    "    df['Generated_answer'] = ''\n",
    "\n",
    "\n",
    "#slow (for the time being) but better results due to custom retriever and reranker. Expect a 2x slowdown. If standard takes 15s, this will take 30s. Looking into ways to speed it up\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=hybrid_retriever,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n",
    "\n",
    "def get_answer(question):\n",
    "    response = query_engine.query(question)\n",
    "    return response\n",
    "\n",
    "for index, row in df.iloc[53:].iterrows(): #crashed midway so had to start from 53\n",
    "    if pd.isna(row['Generated_answer']) or row['Generated_answer'] == '':\n",
    "        question = row['Question']\n",
    "        answer = get_answer(question)\n",
    "        df.at[index, 'Generated_answer'] = answer\n",
    "        print(f\"Question: {question}\\nAnswer: {answer}\")\n",
    "    else:\n",
    "        print(f\"Question: {row['Question']}\\nAnswer: {row['Generated_answer']} (Already answered)\")\n",
    "    print(f\"Completed {index+1}/{len(df)} questions\")\n",
    "\n",
    "df.to_csv(\"C:/Users/Adel/Downloads/QA_Updated.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f4e0e97c055c12b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range (7): #CUDA OUT OF MEMORY ERROR. Only way past is to burn the cache down multiple times/restart. Doing it once doesn't do anything \n",
    "    torch.cuda.empty_cache() "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc1524b983c97b68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
