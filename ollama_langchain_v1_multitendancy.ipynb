{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:20.165888Z",
     "start_time": "2024-03-25T21:01:18.932119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bffc4b7fef1d4aadbb77932a3b17f725"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n\\nmodel_id = \"TheBloke/openchat-3.5-0106-GPTQ\"\\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=False, revision=\\'main\\')\\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0)\\nhf = HuggingFacePipeline(pipeline=pipe)\\nabove very slow. ollama is goated + easier to test\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"TheBloke/openchat-3.5-0106-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=False, revision='main')\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, temperature=0)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)\n",
    "above very slow. ollama is goated + easier to test\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:21.674569Z",
     "start_time": "2024-03-25T21:01:21.670279Z"
    }
   },
   "id": "831f622f36ddadd8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "'''\n",
    "To run this you need to install ollama first \n",
    "https://ollama.com/download\n",
    "Run ollama after installation (it wont show up at all as it runs in the background)\n",
    "In terminal of this venv run \"ollama pull openchat\" to use this model. \n",
    "Don't default pip install torch unless you want to watch paint dry. Use the gpu specific install \n",
    "'''\n",
    "\n",
    "local_llm = \"openchat\"\n",
    "#https://ollama.com/library?sort=popular"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:22.139349Z",
     "start_time": "2024-03-25T21:01:22.136665Z"
    }
   },
   "id": "7d66088d3a252faa"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:25.349017Z",
     "start_time": "2024-03-25T21:01:23.344592Z"
    }
   },
   "id": "6bb2138f55ebe899"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:31<00:00, 31.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "loader = DirectoryLoader('C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', glob=\"**/*.pdf\", show_progress=True,loader_cls=UnstructuredFileLoader, recursive=True)\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:01:59.888593Z",
     "start_time": "2024-03-25T21:01:25.350016Z"
    }
   },
   "id": "6a959ce7d8dc058c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#langchain bastards have progress bar class for every other embedding but not fastembed (⩺_⩹)\n",
    "qdrant_store = Qdrant.from_documents(\n",
    "    location=\":memory:\", #temp memory im just using for testing purposes. not suitable for production but fine enough for demo\n",
    "    collection_name=\"AWS_TEST\",\n",
    "    documents=texts,\n",
    "    embedding=embeddings\n",
    ")\n",
    "retriever = qdrant_store.as_retriever()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e31913543cf888"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.76s/it]\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.05s/it]\n",
      "100%|██████████| 1/1 [00:26<00:00, 26.55s/it]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.11s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define roles\n",
    "roles = ['roleX', 'roleY', 'roleZ', 'admin']\n",
    "role_files = {\n",
    "    'roleX': 'C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', #change the path to files for a speciefic role\n",
    "    'roleY': 'C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', #change the path to files for a speciefic role\n",
    "    'roleZ': 'C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', #change the path to files for a speciefic role\n",
    "    'admin': 'C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', #change the path to files for a speciefic role\n",
    "}\n",
    "\n",
    "# Load docs for each role to Qdrant\n",
    "for role, files in role_files.items():\n",
    "    loader = DirectoryLoader('C:/Users/skfrqt/Desktop/jk/mil6/docs/aws-documentation-main', glob=\"**/*.pdf\", show_progress=True,loader_cls=UnstructuredFileLoader, recursive=True)\n",
    "    documents = loader.load()\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    qdrant_store = Qdrant.from_documents(\n",
    "        location=\":memory:\",\n",
    "        collection_name=role,\n",
    "        documents=texts,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "\n",
    "# Ask user his role\n",
    "user_role = input('What role are you: ' + ', '.join(roles) + '\\n')\n",
    "\n",
    "# Check if role exists\n",
    "if user_role not in roles:\n",
    "    print('Invalid role. Please try again.')\n",
    "else:\n",
    "    # create retriever for user\n",
    "    retriever = qdrant_store.as_retriever(collection_name=user_role)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T01:42:58.266250Z",
     "start_time": "2024-03-26T01:35:06.222653Z"
    }
   },
   "id": "9dab0068fce883d2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Dict, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:03:21.546243Z",
     "start_time": "2024-03-25T21:03:21.542605Z"
    }
   },
   "id": "b51af60ae7672548",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "### Nodes ###\n",
    "\n",
    "#TODO: Use multiple models for different tasks. For example, use a model that is good at generating questions for the transform_query node. Will reduce inference time and improve performance\n",
    "#https://ollama.com/library?sort=popular\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    # print(\"---RETRIEVE---\")\n",
    "    # state_dict = state[\"keys\"]\n",
    "    # question = state_dict[\"question\"]\n",
    "    # documents = retriever.get_relevant_documents(question)\n",
    "    # return {\"keys\": {\"documents\": documents, \"question\": question}}\n",
    "#   change the retrieve func \n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question}}\n",
    "\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "        input_variables=[\"question\",\"context\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": d.page_content,\n",
    "            }\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "\n",
    "    return {\"keys\": {\"documents\": filtered_docs, \"question\": question}}\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "    \n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n \n",
    "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n \n",
    "        Here is the initial question:\n",
    "        \\n ------- \\n\n",
    "        {question} \n",
    "        \\n ------- \\n\n",
    "        Formulate an improved question:\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": better_question}}\n",
    "\n",
    "def prepare_for_final_grade(state):\n",
    "    \"\"\"\n",
    "    Passthrough state for final grade.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The current graph state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---FINAL GRADE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "    }\n",
    "\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: TRANSFORM QUERY---\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "    print(\"---GRADE GENERATION vs DOCUMENTS---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "        Here are the facts:\n",
    "        \\n ------- \\n\n",
    "        {documents} \n",
    "        \\n ------- \\n\n",
    "        Here is the answer: {generation}\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "        input_variables=[\"generation\", \"documents\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    score = chain.invoke({\"generation\": generation, \"documents\": documents})\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: SUPPORTED, MOVE TO FINAL GRADE---\")\n",
    "        return \"supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT SUPPORTED, GENERATE AGAIN---\")\n",
    "        return \"not supported\"\n",
    "\n",
    "def grade_generation_v_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation addresses the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    generation = state_dict[\"generation\"]\n",
    "\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "        Here is the answer:\n",
    "        \\n ------- \\n\n",
    "        {generation} \n",
    "        \\n ------- \\n\n",
    "        Here is the question: {question}\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "        Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "        input_variables=[\"generation\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    score = chain.invoke({\"generation\": generation, \"question\": question})\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: USEFUL---\")\n",
    "        return \"useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: NOT USEFUL---\")\n",
    "        return \"not useful\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T21:03:21.763282Z",
     "start_time": "2024-03-25T21:03:21.547241Z"
    }
   },
   "id": "fe671c86267f2325"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"prepare_for_final_grade\", prepare_for_final_grade)  # passthrough\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents,\n",
    "    {\n",
    "        \"supported\": \"prepare_for_final_grade\",\n",
    "        \"not supported\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"prepare_for_final_grade\",\n",
    "    grade_generation_v_question,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2ba8a3f1e0136e3",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63437cbf600c5b64"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='iv\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '18a91f0f4a6a4ea68803ac099dca9563', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='\"ses:List*\", \"sns:ListTopics\", \"ses:Describe*\", \"s3:List*\" ], \"Resource\": \"*\" } ] }\\n\\nIn the preceding policy example, replace region with the name of an AWS Region, replace accountId with your AWS account ID, and replace projectId with the ID of the Amazon Pinpoint project that you want to provide access to.\\n\\nAmazon Pinpoint SMS and Voice API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:Get*\", \"sms-voice:List*\", ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nAmazon Pinpoint Email API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint Email API:\\n\\n{\\n\\n252\\n\\nAmazon Pinpoint Developer Guide Amazon Pinpoint API Actions\\n\\n\"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants full access to the Amazon SES API.\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint Email API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:Describe*\", \"ses:Get*\", \"ses:List*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants read-only access to the Amazon SES API.\\n\\nAmazon Pinpoint API Actions', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '8c66a9d3d5764f329d3e7ee2eeb96f85', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iii\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': 'daa2d5836d43491aa0b2bcce3bbf2799', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='{ \"ChannelType\": \"APNS\", \"Address\": \"1a2b3c4d5e6f7g8h9i0j1k2l3m4n5o6p7q8r9s0t1u2v3w4x5y6z7a8b9c0d1e2f\", \"EndpointStatus\": \"ACTIVE\", \"OptOut\": \"NONE\", \"RequestId\": \"b720cfa8-6924-11e8-aeda-0b22e0b0fa59\", \"Location\": { \"Latitude\": 47.6, \"Longitude\": -122.3, \"PostalCode\": \"98121\", \"City\": \"Seattle\", \"Country\": \"US\" }, \"Demographic\": {\\n\\n164\\n\\nAmazon Pinpoint Developer Guide Related Information\\n\\n\"Make\": \"apple\", \"Model\": \"iPhone\", \"ModelVersion\": \"8\", \"Timezone\": \"America/Los_Angeles\", \"AppVersion\": \"1.0\", \"Platform\": \"ios\", \"PlatformVersion\": \"11.3.1\" }, \"EffectiveDate\": \"2018-06-06T00:58:19.865Z\", \"Attributes\": { \"interests\": [ \"technology\", \"music\", \"travel\" ] }, \"Metrics\": { \"music_interest_level\": 6, \"travel_interest_level\": 4, \"technology_interest_level\": 9 }, \"User\": {}, \"ApplicationId\": \"application_id\", \"Id\": \"example_endpoint\", \"CohortId\": \"39\", \"CreationDate\": \"2018-06-06T00:58:19.865Z\" }\\n\\nRelated Information\\n\\nFor more information about the Endpoint resource in the Amazon Pinpoint API, see Endpoint in the Amazon Pinpoint API Reference.\\n\\nExporting Endpoints from Amazon Pinpoint\\n\\nTo get all of the information that Amazon Pinpoint has about your audience, you can export the endpoint deﬁnitions that belong to a project. When you export, Amazon Pinpoint places the endpoint deﬁnitions in an Amazon S3 bucket that you specify. Exporting endpoints is useful when you want to:', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '200a57269e474c3fa07759905e30696e', '_collection_name': 'AWS_TEST'})],\n",
      "  'question': 'Is Amazon EBS encryption available on M3 instances?'}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'question': 'Is Amazon EBS encryption available on M3 instances?'}\n",
      "'\\n---\\n'\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY---\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content=\"Note You need to provide both the access key ID and the secret access key in a later step in this tutorial. This is the only time that you're able to view the secret access key, so you should copy it and save it in a safe location.\\n\\nStep 2: Set Up Postman\\n\\nNow that you've created an IAM user account that's able to access the Amazon Pinpoint API, you can set up Postman. In this section, you create one or more environments in Postman. Next, you import a collection that contains a request template for each of the operations in the Amazon Pinpoint API.\\n\\nStep 2.1: Create Postman Environments\\n\\nIn Postman, an environment is a set of variables that are stored as key-value pairs. You can use environments to quickly change the conﬁguration of the requests that you make through Postman, without having to change the API requests themselves.\\n\\nIn this section, you create at least one environment to use with Amazon Pinpoint. Each environment that you create contains a set of variables that are speciﬁc to your account in a single AWS Region. If you use the procedures in this section to create more than one environment, you can easily change between Regions by choosing a diﬀerent environment from the Environment menu in Postman.\\n\\nTo create an environment\\n\\n1.\\n\\nIn Postman, on the File menu, choose New.\\n\\n6\\n\\nAmazon Pinpoint Developer Guide Step 2: Set Up Postman\\n\\n2. On the Create New window, choose Environment.\\n\\n3. On the MANAGE ENVIRONMENTS window, for Environment Name, enter Amazon Pinpoint - Region Name. Replace Region Name with one of the following values:\\n\\nUS East (N. Virginia)\\n\\nUS West (Oregon)\\n\\nAsia Paciﬁc (Mumbai)\\n\\nAsia Paciﬁc (Sydney)\\n\\nEU (Frankfurt)\\n\\nEU (Ireland)\\n\\n4. Create six new variables: endpoint, region, serviceName, accountId, accessKey, and secretAccessKey. Use the following table to determine which value to enter in the Initial Value column for each variable.\\n\\nRegion\\n\\nVariable\\n\\nInitial Value\\n\\nendpoint\", metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '9d9cef81019949bf8107061a94d9b972', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='\"ses:List*\", \"sns:ListTopics\", \"ses:Describe*\", \"s3:List*\" ], \"Resource\": \"*\" } ] }\\n\\nIn the preceding policy example, replace region with the name of an AWS Region, replace accountId with your AWS account ID, and replace projectId with the ID of the Amazon Pinpoint project that you want to provide access to.\\n\\nAmazon Pinpoint SMS and Voice API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:Get*\", \"sms-voice:List*\", ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nAmazon Pinpoint Email API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint Email API:\\n\\n{\\n\\n252\\n\\nAmazon Pinpoint Developer Guide Amazon Pinpoint API Actions\\n\\n\"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants full access to the Amazon SES API.\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint Email API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:Describe*\", \"ses:Get*\", \"ses:List*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants read-only access to the Amazon SES API.\\n\\nAmazon Pinpoint API Actions', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '8c66a9d3d5764f329d3e7ee2eeb96f85', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iv\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '18a91f0f4a6a4ea68803ac099dca9563', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iii\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': 'daa2d5836d43491aa0b2bcce3bbf2799', '_collection_name': 'AWS_TEST'})],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY---\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Can Amazon EBS encryption be enabled on M3 instance types?'}\n",
      "'\\n---\\n'\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content='iv\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '18a91f0f4a6a4ea68803ac099dca9563', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='\"ses:List*\", \"sns:ListTopics\", \"ses:Describe*\", \"s3:List*\" ], \"Resource\": \"*\" } ] }\\n\\nIn the preceding policy example, replace region with the name of an AWS Region, replace accountId with your AWS account ID, and replace projectId with the ID of the Amazon Pinpoint project that you want to provide access to.\\n\\nAmazon Pinpoint SMS and Voice API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:Get*\", \"sms-voice:List*\", ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nAmazon Pinpoint Email API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint Email API:\\n\\n{\\n\\n252\\n\\nAmazon Pinpoint Developer Guide Amazon Pinpoint API Actions\\n\\n\"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants full access to the Amazon SES API.\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint Email API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:Describe*\", \"ses:Get*\", \"ses:List*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants read-only access to the Amazon SES API.\\n\\nAmazon Pinpoint API Actions', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '8c66a9d3d5764f329d3e7ee2eeb96f85', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content=\"Note You need to provide both the access key ID and the secret access key in a later step in this tutorial. This is the only time that you're able to view the secret access key, so you should copy it and save it in a safe location.\\n\\nStep 2: Set Up Postman\\n\\nNow that you've created an IAM user account that's able to access the Amazon Pinpoint API, you can set up Postman. In this section, you create one or more environments in Postman. Next, you import a collection that contains a request template for each of the operations in the Amazon Pinpoint API.\\n\\nStep 2.1: Create Postman Environments\\n\\nIn Postman, an environment is a set of variables that are stored as key-value pairs. You can use environments to quickly change the conﬁguration of the requests that you make through Postman, without having to change the API requests themselves.\\n\\nIn this section, you create at least one environment to use with Amazon Pinpoint. Each environment that you create contains a set of variables that are speciﬁc to your account in a single AWS Region. If you use the procedures in this section to create more than one environment, you can easily change between Regions by choosing a diﬀerent environment from the Environment menu in Postman.\\n\\nTo create an environment\\n\\n1.\\n\\nIn Postman, on the File menu, choose New.\\n\\n6\\n\\nAmazon Pinpoint Developer Guide Step 2: Set Up Postman\\n\\n2. On the Create New window, choose Environment.\\n\\n3. On the MANAGE ENVIRONMENTS window, for Environment Name, enter Amazon Pinpoint - Region Name. Replace Region Name with one of the following values:\\n\\nUS East (N. Virginia)\\n\\nUS West (Oregon)\\n\\nAsia Paciﬁc (Mumbai)\\n\\nAsia Paciﬁc (Sydney)\\n\\nEU (Frankfurt)\\n\\nEU (Ireland)\\n\\n4. Create six new variables: endpoint, region, serviceName, accountId, accessKey, and secretAccessKey. Use the following table to determine which value to enter in the Initial Value column for each variable.\\n\\nRegion\\n\\nVariable\\n\\nInitial Value\\n\\nendpoint\", metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '9d9cef81019949bf8107061a94d9b972', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iii\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': 'daa2d5836d43491aa0b2bcce3bbf2799', '_collection_name': 'AWS_TEST'})],\n",
      "  'question': ' Can Amazon EBS encryption be enabled on M3 instance types?'}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Can Amazon EBS encryption be enabled on M3 instance types?'}\n",
      "'\\n---\\n'\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY---\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "{ 'documents': [ Document(page_content=\"Note You need to provide both the access key ID and the secret access key in a later step in this tutorial. This is the only time that you're able to view the secret access key, so you should copy it and save it in a safe location.\\n\\nStep 2: Set Up Postman\\n\\nNow that you've created an IAM user account that's able to access the Amazon Pinpoint API, you can set up Postman. In this section, you create one or more environments in Postman. Next, you import a collection that contains a request template for each of the operations in the Amazon Pinpoint API.\\n\\nStep 2.1: Create Postman Environments\\n\\nIn Postman, an environment is a set of variables that are stored as key-value pairs. You can use environments to quickly change the conﬁguration of the requests that you make through Postman, without having to change the API requests themselves.\\n\\nIn this section, you create at least one environment to use with Amazon Pinpoint. Each environment that you create contains a set of variables that are speciﬁc to your account in a single AWS Region. If you use the procedures in this section to create more than one environment, you can easily change between Regions by choosing a diﬀerent environment from the Environment menu in Postman.\\n\\nTo create an environment\\n\\n1.\\n\\nIn Postman, on the File menu, choose New.\\n\\n6\\n\\nAmazon Pinpoint Developer Guide Step 2: Set Up Postman\\n\\n2. On the Create New window, choose Environment.\\n\\n3. On the MANAGE ENVIRONMENTS window, for Environment Name, enter Amazon Pinpoint - Region Name. Replace Region Name with one of the following values:\\n\\nUS East (N. Virginia)\\n\\nUS West (Oregon)\\n\\nAsia Paciﬁc (Mumbai)\\n\\nAsia Paciﬁc (Sydney)\\n\\nEU (Frankfurt)\\n\\nEU (Ireland)\\n\\n4. Create six new variables: endpoint, region, serviceName, accountId, accessKey, and secretAccessKey. Use the following table to determine which value to enter in the Initial Value column for each variable.\\n\\nRegion\\n\\nVariable\\n\\nInitial Value\\n\\nendpoint\", metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '9d9cef81019949bf8107061a94d9b972', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='\"ses:List*\", \"sns:ListTopics\", \"ses:Describe*\", \"s3:List*\" ], \"Resource\": \"*\" } ] }\\n\\nIn the preceding policy example, replace region with the name of an AWS Region, replace accountId with your AWS account ID, and replace projectId with the ID of the Amazon Pinpoint project that you want to provide access to.\\n\\nAmazon Pinpoint SMS and Voice API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint SMS and Voice API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"sms-voice:Get*\", \"sms-voice:List*\", ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nAmazon Pinpoint Email API Actions\\n\\nAdministrator Access\\n\\nThe following policy grants full access to the Amazon Pinpoint Email API:\\n\\n{\\n\\n252\\n\\nAmazon Pinpoint Developer Guide Amazon Pinpoint API Actions\\n\\n\"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants full access to the Amazon SES API.\\n\\nRead-Only Access\\n\\nThe following policy allows read-only access to the Amazon Pinpoint Email API:\\n\\n{ \"Version\": \"2018-09-05\", \"Statement\": [ { \"Action\": [ \"ses:Describe*\", \"ses:Get*\", \"ses:List*\" ], \"Effect\": \"Allow\", \"Resource\": \"*\" } ] }\\n\\nNote This policy also grants read-only access to the Amazon SES API.\\n\\nAmazon Pinpoint API Actions', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '8c66a9d3d5764f329d3e7ee2eeb96f85', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iv\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': '18a91f0f4a6a4ea68803ac099dca9563', '_collection_name': 'AWS_TEST'}),\n",
      "                 Document(page_content='iii\\n\\nAmazon Pinpoint Developer Guide', metadata={'source': 'C:\\\\Users\\\\skfrqt\\\\Desktop\\\\jk\\\\mil6\\\\docs\\\\aws-documentation-main\\\\documents\\\\amazon-pinpoint-developer-guide\\\\doc-source\\\\pinpoint-dg.pdf', '_id': 'daa2d5836d43491aa0b2bcce3bbf2799', '_collection_name': 'AWS_TEST'})],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "\"Node 'grade_documents':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Is it possible to enable Amazon EBS encryption on M3 instance '\n",
      "              'types?'}\n",
      "'\\n---\\n'\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY---\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "{ 'documents': [],\n",
      "  'question': ' Can Amazon EBS encryption be enabled on M3 instance types?'}\n",
      "'\\n---\\n'\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the `recursion_limit` config key.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mGraphRecursionError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeys\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquestion\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIs Amazon EBS encryption available on M3 instances?\u001B[39m\u001B[38;5;124m\"\u001B[39m}}\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mapp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Node\u001B[39;49;00m\n\u001B[0;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpprint\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpprint\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNode \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mkey\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m:\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:886\u001B[0m, in \u001B[0;36mPregel.transform\u001B[1;34m(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\u001B[0m\n\u001B[0;32m    874\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtransform\u001B[39m(\n\u001B[0;32m    875\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    876\u001B[0m     \u001B[38;5;28minput\u001B[39m: Iterator[Union[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any], Any]],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    885\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Union[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any], Any]]:\n\u001B[1;32m--> 886\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform_stream_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    887\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    888\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    889\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterrupt_before_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_before_nodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterrupt_after_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_after_nodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    896\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    897\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1743\u001B[0m, in \u001B[0;36mRunnable._transform_stream_with_config\u001B[1;34m(self, input, transformer, config, run_type, **kwargs)\u001B[0m\n\u001B[0;32m   1741\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1742\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m         chunk: Output \u001B[38;5;241m=\u001B[39m context\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;28mnext\u001B[39m, iterator)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m chunk\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m final_output_supported:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:527\u001B[0m, in \u001B[0;36mPregel._transform\u001B[1;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m step \u001B[38;5;241m==\u001B[39m config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecursion_limit\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m--> 527\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m GraphRecursionError(\n\u001B[0;32m    528\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRecursion limit of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecursion_limit\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m reached\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    529\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwithout hitting a stop condition. You can increase the limit\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    530\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby setting the `recursion_limit` config key.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    531\u001B[0m     )\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m debug:\n\u001B[0;32m    534\u001B[0m     print_step_start(step, next_tasks)\n",
      "\u001B[1;31mGraphRecursionError\u001B[0m: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the `recursion_limit` config key."
     ]
    }
   ],
   "source": [
    "inputs = {\"keys\": {\"question\": \"Is Amazon EBS encryption available on M3 instances?\"}}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value['keys']['generation'])  "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T00:27:09.333327Z",
     "start_time": "2024-03-26T00:26:16.339040Z"
    }
   },
   "id": "ebeffc0557275e64"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nNotes:\\nThe QA csv sheet provided has some errors in it (e.g. the question \"What is the maximum number of security groups associated with a load balancer?\" is 5 but the wrong answer expected is 50\\nLlamaindex/langchain huggingfacellm wrapper literally makes inference almost 7x slower vs using a direct pipeline.\\nOllama is godsend but we should use it with linux/wsl for faster inference \\nAlthough llamaindex does have some nice features that langchain doesnt have (at a glance) - we should use both of them in production instead of only one. both of their docs arent very clear on lots of things/are outdated\\nI havent tested performance between chroma and qdrant but at a glance qdrant seems to be faster\\nAbove code is from langchain\\'s example (wouldve saved a months worth of work if we pivoted earlier) - wasted an entire day trying to use with huggingface but decided on ollama like their docs \\nPerformance completely depends on the question asked. If the question is too vague, the model will take a long time to generate a response as it has to make multiple calls to rewrite question/rank response to prevent hallucinations. \\nCurrent benchmark is 25-45s on average. 2 min slowest i\\'ve seen. Needs some more tuning - will see if using a different model for each task will help\\nJust generating an answer on its own without going through workflow takes a couple of seconds\\n\\n(Modular workflow graph - needs some further tuning)\\nStep 1: User asks a question\\nStep 2: Retrieve relevant document\\nStep 3: Grade document relevance\\nStep 4: If document is not relevant go back to step 2 (error here is if all documents are not relevant we get stuck in an \"infinite\" loop but it usually stops after 25 calls - roughly 2 mins)\\nStep 5: Generate answer\\nStep 6: Grade answer. If answer is hallucinated/doesnt match document go back to step 5 (same error as 4 if context doesnt exist)\\nStep 7: If answer is useful, end. If not go back to step 2\\n\\n'"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Notes:\n",
    "The QA csv sheet provided has some errors in it (e.g. the question \"What is the maximum number of security groups associated with a load balancer?\" is 5 but the wrong answer expected is 50\n",
    "Llamaindex/langchain huggingfacellm wrapper literally makes inference almost 7x slower vs using a direct pipeline.\n",
    "Ollama is godsend but we should use it with linux/wsl for faster inference \n",
    "Although llamaindex does have some nice features that langchain doesnt have (at a glance) - we should use both of them in production instead of only one. both of their docs arent very clear on lots of things/are outdated\n",
    "I havent tested performance between chroma and qdrant but at a glance qdrant seems to be faster\n",
    "Above code is from langchain's example (wouldve saved a months worth of work if we pivoted earlier) - wasted an entire day trying to use with huggingface but decided on ollama like their docs \n",
    "Performance completely depends on the question asked. If the question is too vague, the model will take a long time to generate a response as it has to make multiple calls to rewrite question/rank response to prevent hallucinations. \n",
    "Current benchmark is 25-45s on average. 2 min slowest i've seen. Needs some more tuning - will see if using a different model for each task will help\n",
    "Just generating an answer on its own without going through workflow takes a couple of seconds\n",
    "\n",
    "(Modular workflow graph - needs some further tuning)\n",
    "Step 1: User asks a question\n",
    "Step 2: Retrieve relevant document\n",
    "Step 3: Grade document relevance\n",
    "Step 4: If document is not relevant go back to step 2 (error here is if all documents are not relevant we get stuck in an \"infinite\" loop but it usually stops after 25 calls - roughly 2 mins)\n",
    "Step 5: Generate answer\n",
    "Step 6: Grade answer. If answer is hallucinated/doesnt match document go back to step 5 (same error as 4 if context doesnt exist)\n",
    "Step 7: If answer is useful, end. If not go back to step 2\n",
    "\n",
    "'''\n",
    "\n",
    "#TODO: Use linux/wsl with vLLM for faster inference. Should be almost 2x faster than Windows from some readings?\n",
    "#TODO: Use llamaindex for some tasks and langchain for others. Both have their own strengths and weaknesses. Both have horrible docs\n",
    "#TODO: Host qdrant on a server\n",
    "#TODO: Figure out metadata for documents. Useful for filtering out irrelevant/old answers - should sped up & reduce hallucinations\n",
    "#TODO: Add preprocessing & postprocessing to the workflow. Will be useful for filtering out irrelevant/old answers - should sped up & reduce hallucinations\n",
    "#TODO: Test CRAG performance against current self-rag implementation\n",
    "#TODO: Add chat history for multiple-shot reasoning\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:21:47.054792Z",
     "start_time": "2024-03-25T19:21:47.050554Z"
    }
   },
   "id": "6c776821eb27a86c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39d13517b6bdee3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
